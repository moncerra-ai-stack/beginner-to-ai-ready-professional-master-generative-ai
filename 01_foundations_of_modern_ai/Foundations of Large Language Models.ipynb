{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Phi-3 Instruct ‚Äî Chat Completion Demo  \n",
    "A clean, beginner-friendly, line-by-line explanation.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Overview  \n",
    "In this notebook, we will:  \n",
    "- Load the **Phi-3 Mini Instruct** model  \n",
    "- Build a reusable chat function  \n",
    "- Understand each step with clear explanations  \n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Install & Import Libraries\n",
    "\n",
    "```python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_phi3_instruct(model_id: str = \"microsoft/Phi-3-mini-4k-instruct\"):\n",
    "    \"\"\"\n",
    "    Load the Phi-3 instruct model and its tokenizer.\n",
    "    Returns (tokenizer, model) placed on an appropriate device.\n",
    "    \"\"\"\n",
    "    # Detect GPU or use CPU as fallback\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    # Load the model and move it to device\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=\"auto\",\n",
    "        trust_remote_code=False,\n",
    "    ).to(device)\n",
    "\n",
    "    return tokenizer, model, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_model(tokenizer, model, device, chat_history, max_new_tokens: int = 128):\n",
    "    \"\"\"\n",
    "    Given a list of chat messages, run one completion and return the model's reply text.\n",
    "    \"\"\"\n",
    "    # Convert structured messages into a chat-formatted text prompt\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        chat_history,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    # Tokenize prompt and move to GPU/CPU\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate the model‚Äôs continuation\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    # Extract only the newly generated tokens\n",
    "    generated_ids = output_ids[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    reply = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    return reply.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Load Phi-3 model and tokenizer\n",
    "    tokenizer, model, device = load_phi3_instruct()\n",
    "\n",
    "    # Step 2: Chat-style messages\n",
    "    conversation = [\n",
    "        {\"role\": \"user\", \"content\": \"What is Generative AI.\"}\n",
    "    ]\n",
    "\n",
    "    # Step 3: Generate model response\n",
    "    answer = chat_with_model(tokenizer, model, device, conversation, max_new_tokens=100)\n",
    "\n",
    "    # Step 4: Print output\n",
    "    print(answer)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPCWg08aO4e8NWQuYCK5ppF",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
