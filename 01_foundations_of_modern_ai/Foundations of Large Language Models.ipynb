{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phi-3 Instruct — Chat Completion Demo  \n",
    "\n",
    "---\n",
    "\n",
    "## Overview  \n",
    "In this notebook, we will:  \n",
    "- Load the **Phi-3 Mini Instruct** model  \n",
    "- Build a reusable chat function  \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_phi3_instruct(model_id: str = \"microsoft/Phi-3-mini-4k-instruct\"):\n",
    "    \"\"\"\n",
    "    Load the Phi-3 instruct model and its tokenizer.\n",
    "    Returns (tokenizer, model) placed on an appropriate device.\n",
    "    \"\"\"\n",
    "    # Detect GPU or use CPU as fallback\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    # Load the model and move it to device\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=\"auto\",\n",
    "        trust_remote_code=False,\n",
    "    ).to(device)\n",
    "\n",
    "    return tokenizer, model, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_model(tokenizer, model, device, chat_history, max_new_tokens: int = 128):\n",
    "    \"\"\"\n",
    "    Given a list of chat messages, run one completion and return the model's reply text.\n",
    "    \"\"\"\n",
    "    # Convert structured messages into a chat-formatted text prompt\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        chat_history,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    # Tokenize prompt and move to GPU/CPU\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate the model’s continuation\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    # Extract only the newly generated tokens\n",
    "    generated_ids = output_ids[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    reply = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    return reply.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Load Phi-3 model and tokenizer\n",
    "    tokenizer, model, device = load_phi3_instruct()\n",
    "\n",
    "    # Step 2: Chat-style messages\n",
    "    conversation = [\n",
    "        {\"role\": \"user\", \"content\": \"What is Generative AI.\"}\n",
    "    ]\n",
    "\n",
    "    # Step 3: Generate model response\n",
    "    answer = chat_with_model(tokenizer, model, device, conversation, max_new_tokens=100)\n",
    "\n",
    "    # Step 4: Print output\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer : How words are broken into subword units by the trained tokenizer of an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# initialize tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# load DistilBERT encoder\n",
    "encoder = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# example text\n",
    "sample = \"Master Generative AI\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = tok(sample, return_tensors='pt')\n",
    "\n",
    "# encode text (disable token type ids)\n",
    "encoded = tok(\n",
    "    sample,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False   # <--- important fix\n",
    ")\n",
    "\n",
    "# forward pass\n",
    "result = encoder(**encoded).last_hidden_state\n",
    "\n",
    "print(result.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tokens['input_ids'][0]:\n",
    "    print(tok.decode(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings : Where meaning of sentences is abstracted within a static vector of fixed size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load a pretrained model (small, fast, great for demos)\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Define sentences to embed\n",
    "sentences = [\n",
    "    \"Artificial Intelligence is transforming every industry.\",\n",
    "    \"Machine learning enables computers to learn patterns from data.\",\n",
    "    \"Generative AI is the future.\"\n",
    "]\n",
    "\n",
    "# 3. Generate vector embeddings\n",
    "embeddings = model.encode(sentences, convert_to_numpy=True)\n",
    "\n",
    "# 4. Show results\n",
    "print(\"Number of sentences:\", len(sentences))\n",
    "print(\"Embedding vector shape:\", embeddings.shape)\n",
    "print(\"\\nFirst sentence embedding (first 10 values):\")\n",
    "print(embeddings[0][:10])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPCWg08aO4e8NWQuYCK5ppF",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
